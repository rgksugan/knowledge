{"componentChunkName":"component---node-modules-gatsby-theme-kb-src-templates-topic-js","path":"/3-resources/books/AI-Engineering","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"tags\": \"non-fiction\",\n  \"type\": \"book\",\n  \"author\": \"Chip Huyen\",\n  \"title\": \"AI Engineering\",\n  \"sub-title\": \"Building Applications with Foundation Models\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"AI Engineering\"), mdx(\"p\", null, \"by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/Chip-Huyen\",\n    \"title\": \"Chip Huyen\"\n  }, \"[[Chip Huyen]]\"), \"\"), mdx(\"h2\", null, \"Highlights\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Evaluation is one of the hardest, if not the hardest, challenges of AI engineering.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Our success depends on our ability to filter and digest useful information.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"It might take a weekend to build a demo but months, and even years, to build a product.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"AI engineering is just software engineering with AI models thrown in the stack.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"An advantage that full-stack engineers have over traditional ML engineers is their ability to quickly turn ideas into demos, get feedback, and iterate.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Prompt engineering is about getting AI models to express the desirable behaviors from the input alone, without changing the model weights.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"With foundation models, where many teams use the same model, differentiation must be gained through the application development process.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"In traditional ML engineering, most use cases are close-ended\\u2014a model\\u2019s output can only be among predefined values. For example, spam classification with only two possible outputs, \\u201Cspam\\u201D and \\u201Cnot spam\\u201D, is close-ended. Foundation models, however, are open-ended. Annotating open-ended queries is much harder than annotating close-ended queries\\u2014it\\u2019s easier to determine whether an email is spam than to write an essay. So data annotation is a much bigger challenge for AI engineering.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Pre-training also takes a long time to do. A small mistake during pre-training can incur a significant financial loss and set back the project significantly. Due to the resource-intensive nature of pre-training, this has become an art that only a few practice. Those with expertise in pre-training large models, however, are heavily sought after.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Out of all training steps, pre-training is often the most resource-intensive by a long shot.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Training always involves changing model weights, but not all changes to model weights constitute training.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"An AI model is only as good as the data it was trained on.\")), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Training on more data often requires more compute resources and doesn\\u2019t always lead to better performance.\")), mdx(\"h2\", null, \"Contents\"), mdx(\"h2\", null, \"The Book in 3 Sentences\"), mdx(\"h2\", null, \"Who Should Read It?\"), mdx(\"h2\", null, \"My Top 3 Quotes\"), mdx(\"h2\", null, \"Notes\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Lindy\\u2019s Law\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Given a query, the quality of a model\\u2019s response depends on the following aspects (outside of the model\\u2019s generation setting):\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The instructions for how the model should behave.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The context the model can use to respond to the query.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The model itself.\")))), mdx(\"h3\", null, \"Introduction to Building AI Applications with Foundation Models\"), mdx(\"h4\", null, \"The Rise of AI Engineering\"), mdx(\"h5\", null, \"From Language Models to Large Language Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"A language model encodes statistical information about one or more languages.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The basic unit of a language is token.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The process of breaking the original text into tokens is called tokenization.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The set of all tokens a model can work with is the model's vocabulary.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"There are two main types of language models:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Masked language models\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Masked language models are commonly used for non-generative tasks such as sentiment analysis and text classification. They are also useful for tasks requiring an understanding of the overall context, such as code debugging, where a model needs to understand both the preceding and following code to identify errors.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Autoregressive language models.\", mdx(\"img\", {\n    style: {\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"width\": \"80%\"\n    },\n    src: \"/knowledge/41b103788c856075123e803d5c3393eb/masked vs autoregressive.png\",\n    alt: \"Masked vs Autoregressive LM\"\n  })))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The outputs of language models are open-ended. A language model can use its fixed, finite vocabulary to construct infinite possible outputs. A model that can generate open-ended outputs is called generative, hence the term generative AI.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"It\\u2019s important to note that completions are predictions, based on probabilities, and not guaranteed to be correct. This probabilistic nature of language models makes them both so exciting and frustrating to use.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"While completion is powerful, completion isn\\u2019t the same as engaging in a conversation.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Language modeling is just one of many ML algorithms. There are also models for object detection, topic modeling, recommender systems, weather forecasting, stock price prediction, etc.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The answer is that language models can be trained using self-supervision, while many other models require supervision. Supervision refers to the process of training ML algorithms using labeled data, which can be expensive and slow to obtain. Self-supervision helps overcome this data labeling bottleneck to create larger datasets for models to learn from, effectively allowing models to scale up.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"A drawback of supervision is that data labeling is expensive and time-consuming.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Self-supervision differs from unsupervision. In self-supervised learning, labels are inferred from the input data. In unsupervised learning, you don\\u2019t need labels at all.\"))), mdx(\"h5\", null, \"From Large Language Models to Foundation Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"A model that can work with more than one data modality is also called a multimodal model. A generative multimodal model is also called a large multimodal model(LMM).\"), mdx(\"img\", {\n    style: {\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"width\": \"80%\"\n    },\n    src: \"/knowledge/478c417647a2f2d21f20f4552e71318b/multi-modal-model.png\",\n    alt: \"Multi modal model\"\n  })), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Foundation models also mark the transition from task-specific models to general-purpose models. Previously, models were often developed for specific tasks, such as sentiment analysis or translation. A model trained for sentiment analysis wouldn\\u2019t be able to do translation, and vice versa.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Prompt engineering, RAG, and finetuning are three very common AI engineering techniques that you can use to adapt a model to your needs.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Foundation models make it cheaper to develop AI applications and reduce time to market.\"))), mdx(\"h5\", null, \"From Foundation Models to AI Engineering\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AI engineering refers to the process of building applications on top of foundation models.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Teaching AI to behave is the fastest-growing career skill.\")), mdx(\"h4\", null, \"Foundation Model Use Cases\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Internal applications help companies develop their AI engineering expertise while minimizing the risks associated with data privacy, compliance, and potential catastrophic failures.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Classification tasks are easier to evaluate, which makes their risks easier to estimate.\"))), mdx(\"h5\", null, \"Coding\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Extracting structured data from web pages and PDFs (AgentGPT) Converting English to code (DB-GPT, SQL Chat, PandasAI)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Given a design or a screenshot, generating code that will render into a website that looks like the given image (screenshot-to-code, draw-a-ui) Translating from one programming language or framework to another (GPT-Migrate, AI Code Translator) Writing documentation (Autodoc) Creating tests (PentestGPT) Generating commit messages (AI Commits)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"In a leaked recording, AWS CEO Matt Garman shared that in the near future, most developers will stop coding.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"In my conversations with developers of AI coding tools, many told me that they\\u2019ve noticed that AI is much better at frontend development than backend development.\"))), mdx(\"h5\", null, \"Image and Video Production\"), mdx(\"h5\", null, \"Writing\"), mdx(\"h5\", null, \"Education\"), mdx(\"h5\", null, \"Conversational Bots\"), mdx(\"h5\", null, \"Information Aggregation\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Many people believe that our success depends on our ability to filter and digest useful information. However, keeping up with emails, Slack messages, and news can sometimes be overwhelming. Luckily, AI came to the rescue. AI has proven to be capable of aggregating information and summarizing it.\")), mdx(\"h5\", null, \"Data Organization\"), mdx(\"h5\", null, \"Workflow Automation\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AIs that can plan and use tools are called agents.\")), mdx(\"h4\", null, \"Planning AI Applications\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The more critical AI is to the application, the more accurate and reliable the AI part has to be. People are more accepting of mistakes when AI isn't core to the application.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Involving humans in AI\\u2019s decision-making processes is called human-in-the-loop.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Crawl means human involvement is mandatory. Walk means AI can directly interact with internal employees. Run means increased automation, potentially including direct AI interactions with external users.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If you\\u2019re selling AI applications as standalone products, it\\u2019s important to consider their defensibility. The low entry barrier is both a blessing and a curse. If something is easy for you to build, it\\u2019s also easy for your competitors. What moats do you have to defend your product?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"One general partner at a major VC firm told me that she\\u2019s seen many startups whose entire products could be a feature for Google Docs or Microsoft Office. If their products take off, what would stop Google or Microsoft from allocating three engineers to replicate these products in two weeks?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In AI, there are generally three types of competitive advantages: technology, data, and distribution\\u2014the ability to bring your product in front of users. With foundation models, the core technologies of most companies will be similar. The distribution advantage likely belongs to big companies.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The data advantage is more nuanced. Big companies likely have more existing data. However, if a startup can get to market first and gather sufficient usage data to continually improve their products, data will be their moat.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Latency metrics including TTFT (time to first token), TPOT (time per output token), and total latency.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You\\u2019ll have to constantly be on your guard and run a cost-benefit analysis of each technology investment. The best option today might turn into the worst option tomorrow. You may decide to build a model in-house because it seems cheaper than paying for model providers, only to find out after three months that model providers have dropped their prices in half, making in-house the expensive option. You might invest in a third-party solution and tailor your infrastructure around it, only for the provider to go out of business after failing to secure funding.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Some changes can even be fatal. For example, regulations around intellectual property (IP) and AI usage are still evolving. If you build your product on top of a model trained using other people\\u2019s data, can you be certain that your product\\u2019s IP will always belong to you? Many IP-heavy companies I\\u2019ve talked to, such as game studios, hesitate to use AI for fear of losing their IPs later on.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Massive Multitask Language Understanding (MMLU), a popular foundation model benchmark.\")), mdx(\"h4\", null, \"The AI Engineering Stack\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"There are three layers to any AI application stack:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"application development,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"model development,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"infrastructure.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Without foundation models, you have to train your own models for your applications. With AI engineering, you use a model someone else has trained for you. This means that AI engineering focuses less on modeling and training, and more on model adaptation.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AI engineering works with models that are bigger, consume more compute resources, and incur higher latency than traditional ML engineering.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AI engineering works with models that can produce open-ended outputs. Open-ended outputs give models the flexibility to be used for more tasks, but they are also harder to evaluate. This makes evaluation a much bigger problem in AI engineering.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In short, AI engineering differs from ML engineering in that it\\u2019s less about model development and more about adapting and evaluating models.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Prompt-based techniques, which include prompt engineering, adapt a model without updating the model weights.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You adapt a model by giving it instructions and context instead of changing the model itself. Prompt engineering is easier to get started and requires less data. Many successful applications have been built with just prompt engineering. Its ease of use allows you to experiment with more models, which increases your chance of finding a model that is unexpectedly good for your applications. However, prompt engineering might not be enough for complex tasks or applications with strict performance requirements.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Finetuning, on the other hand, requires updating model weights. You adapt a model by making changes to the model itself. In general, finetuning techniques are more complicated and require more data, but they can improve your model\\u2019s quality, latency, and cost significantly. Many things aren\\u2019t possible without changing model weights, such as adapting the model to a new task it wasn\\u2019t exposed to during training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Developing ML models requires specialized ML knowledge. It requires knowing different types of ML algorithms (such as clustering, logistic regression, decision trees, and collaborative filtering) and neural network architectures (such as feedforward, recurrent, convolutional, and transformer). It also requires understanding how a model learns, including concepts such as gradient descent, loss function, regularization, etc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Modeling and training refers to the process of coming up with a model architecture, training it, and finetuning it. Examples of tools in this category are Google\\u2019s TensorFlow, Hugging Face\\u2019s Transformers, and Meta\\u2019s PyTorch.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"There are certain types of ML algorithms.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"clustering\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"logistic regression\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"decision trees\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"collaborative filtering\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"neural networks\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"feedforward\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"recurrent\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"convolutional\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"transformer\")))))), mdx(\"h3\", null, \"Understanding Foundation Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"In general, however, differences in foundation models can be traced back to decisions about training data, model architecture and size, and how they are post-trained to align with human preferences.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"While most people understand the impact of training on a model\\u2019s performance, the impact of sampling is often overlooked. Sampling is how a model chooses an output from all possible options.\"))), mdx(\"h4\", null, \"Training Data\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"An AI model is only as good as the data it was trained on.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Training on more data often requires more compute resources and doesn\\u2019t always lead to better performance.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A model's inference latency and cost is proportional to the number of tokens in the input and response.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For the MASSIVE dataset, the median token length in English is 7, but the median length in Hindi is 32, and in Burmesem it's a whooping 72, which is ten times longer than in English. Assuming that the time it takes to generate a token is the same in all languages, GPT-4 takes approxiamtely ten times longer in Burmese than in English for the same content. For APIs that charge by token usage, Burmese costs ten times more than English.\")), mdx(\"h4\", null, \"Modeling\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Transformer architecture uses attention mechanism.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Much of AI progress in recent years can be attributed to increased model size. It\\u2019s hard to talk about foundation models without talking about their number of parameters. The number of parameters is usually appended at the end of a model name. For example, Llama-13B refers to the version of Llama, a model family developed by Meta, with 13 billion parameters.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In general, increasing a model\\u2019s parameters increases its capacity to learn, resulting in better models. Given two models of the same model family, the one with 13 billion parameters is likely to perform much better than the one with 7 billion parameters.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"As the community better understands how to train large models, newer-generation models tend to outperform older-generation models of the same size. For example, Llama 3-8B (2024) outperforms even Llama 2-70B (2023) on the MMLU benchmark.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The number of parameters helps us estimate the compute resources needed to train and run this model. For example, if a model has 7 billion parameters, and each parameter is stored using 2 bytes (16 bits), then we can calculate that the GPU memory needed to do inference using this model will be at least 14 billion bytes (14 GB).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The number of parameters can be misleading if the model is sparse. A sparse model has a large percentage of zero-value parameters. A 7B-parameter model that is 90% sparse only has 700 million non-zero parameters. Sparsity allows for more efficient data storage and computation. This means that a large sparse model can require less compute than a small dense model.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A larger model can also underperform a smaller model if it\\u2019s not trained on enough data. Imagine a 13B-param model trained on a dataset consisting of a single sentence: \\u201CI like pineapples.\\u201D This model will perform much worse than a much smaller model trained on more data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Quantity, quality, and diversity are the three golden goals for training data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In summary, three numbers signal a model\\u2019s scale:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Number of parameters, which is a proxy for the model\\u2019s learning capacity.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Number of tokens a model was trained on, which is a proxy for how much a model learned.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Number of FLOPs, which is a proxy for the training cost.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Model performance depends on the model size and the dataset size. Bigger models and bigger datasets require more compute. Compute costs money.\")), mdx(\"h4\", null, \"Sampling\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A temperature of 0.7 is often recommended for creative use cases, as it balances creativity and predictability.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The opposite of probabilistic is deterministic, when the outcome can be determined without any random variation.\")), mdx(\"h3\", null, \"Evaluation Methodology\"), mdx(\"h2\", null, \"Abbreviations\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RNN - Recurrent Neural Networks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MLP - Multi Layer Perceptron\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SSM - State Space Models\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"TPU - Tensor Processing Units\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MoE - Mixture of Experts\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"FLOP - Floating Point Operation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SFT - Supervised Fine Tuning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RL - Reinforcement Learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RLHF - Reinforcement Learning from Human feedback\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"DPO - Direct Preference Optimization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RLAIF - Reinforcement Learning from AI Feedback\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"LMSYS - Large Model Systems Organization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"PPO - Proximal Policy Optimization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"NLG - Natural Language Generation\")), mdx(\"h2\", null, \"Terms\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Epoch - A pass through a dataset.\")), mdx(\"h2\", null, \"Bibliography\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"AI Engineering","private":false},"tableOfContents":{"items":[{"url":"#ai-engineering","title":"AI Engineering","items":[{"url":"#highlights","title":"Highlights"},{"url":"#contents","title":"Contents"},{"url":"#the-book-in-3-sentences","title":"The Book in 3 Sentences"},{"url":"#who-should-read-it","title":"Who Should Read It?"},{"url":"#my-top-3-quotes","title":"My Top 3 Quotes"},{"url":"#notes","title":"Notes","items":[{"url":"#introduction-to-building-ai-applications-with-foundation-models","title":"Introduction to Building AI Applications with Foundation Models","items":[{"url":"#the-rise-of-ai-engineering","title":"The Rise of AI Engineering","items":[{"url":"#from-language-models-to-large-language-models","title":"From Language Models to Large Language Models"},{"url":"#from-large-language-models-to-foundation-models","title":"From Large Language Models to Foundation Models"},{"url":"#from-foundation-models-to-ai-engineering","title":"From Foundation Models to AI Engineering"}]},{"url":"#foundation-model-use-cases","title":"Foundation Model Use Cases","items":[{"url":"#coding","title":"Coding"},{"url":"#image-and-video-production","title":"Image and Video Production"},{"url":"#writing","title":"Writing"},{"url":"#education","title":"Education"},{"url":"#conversational-bots","title":"Conversational Bots"},{"url":"#information-aggregation","title":"Information Aggregation"},{"url":"#data-organization","title":"Data Organization"},{"url":"#workflow-automation","title":"Workflow Automation"}]},{"url":"#planning-ai-applications","title":"Planning AI Applications"},{"url":"#the-ai-engineering-stack","title":"The AI Engineering Stack"}]},{"url":"#understanding-foundation-models","title":"Understanding Foundation Models","items":[{"url":"#training-data","title":"Training Data"},{"url":"#modeling","title":"Modeling"},{"url":"#sampling","title":"Sampling"}]},{"url":"#evaluation-methodology","title":"Evaluation Methodology"}]},{"url":"#abbreviations","title":"Abbreviations"},{"url":"#terms","title":"Terms"},{"url":"#bibliography","title":"Bibliography"}]}]},"outboundReferences":[],"inboundReferences":[]},"fields":{"slug":"/3-resources/books/AI-Engineering","title":"AI Engineering"}}},"pageContext":{"id":"50903434-6baf-555b-8e1b-f4e414250f2e","refWordMdxSlugDict":{},"tocTypes":["sidebar"]}},"staticQueryHashes":["2221750479","2380733210","2768355698","63159454","847517413"]}