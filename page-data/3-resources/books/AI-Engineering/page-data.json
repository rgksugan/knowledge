{"componentChunkName":"component---node-modules-gatsby-theme-kb-src-templates-topic-js","path":"/3-resources/books/AI-Engineering","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"non-fiction\",\n  \"type\": \"book\",\n  \"author\": \"Chip Huyen\",\n  \"title\": \"AI Engineering\",\n  \"sub-title\": \"Building Applications with Foundation Models\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"AI Engineering\"), mdx(\"p\", null, \"by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/Chip-Huyen\",\n    \"title\": \"Chip Huyen\"\n  }, \"[[Chip Huyen]]\"), \"\"), mdx(\"h2\", null, \"Highlights\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Evaluation is one of the hardest, if not the hardest, challenges of AI engineering.\")), mdx(\"h2\", null, \"Contents\"), mdx(\"h2\", null, \"The Book in 3 Sentences\"), mdx(\"h2\", null, \"Who Should Read It?\"), mdx(\"h2\", null, \"My Top 3 Quotes\"), mdx(\"h2\", null, \"Notes\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Lindy\\u2019s Law\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Given a query, the quality of a model\\u2019s response depends on the following aspects (outside of the model\\u2019s generation setting):\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The instructions for how the model should behave.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The context the model can use to respond to the query.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The model itself.\")))), mdx(\"h3\", null, \"Introduction to Building AI Applications with Foundation Models\"), mdx(\"h4\", null, \"The Rise of AI Engineering\"), mdx(\"h5\", null, \"From Language Models to Large Language Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The process of breaking the original text into tokens is called tokenization.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"There are two main types of language models:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Masked language models\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Masked language models are commonly used for non-generative tasks such as sentiment analysis and text classification. They are also useful for tasks requiring an understanding of the overall context, such as code debugging, where a model needs to understand both the preceding and following code to identify errors.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Autoregressive language models.\", mdx(\"img\", {\n    style: {\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"width\": \"80%\"\n    },\n    src: \"/knowledge/41b103788c856075123e803d5c3393eb/masked vs autoregressive.png\",\n    alt: \"Masked vs Autoregressive LM\"\n  })))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The outputs of language models are open-ended. A language model can use its fixed, finite vocabulary to construct infinite possible outputs. A model that can generate open-ended outputs is called generative, hence the term generative AI.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"It\\u2019s important to note that completions are predictions, based on probabilities, and not guaranteed to be correct. This probabilistic nature of language models makes them both so exciting and frustrating to use.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"While completion is powerful, completion isn\\u2019t the same as engaging in a conversation.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Language modeling is just one of many ML algorithms. There are also models for object detection, topic modeling, recommender systems, weather forecasting, stock price prediction, etc.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The answer is that language models can be trained using self-supervision, while many other models require supervision. Supervision refers to the process of training ML algorithms using labeled data, which can be expensive and slow to obtain. Self-supervision helps overcome this data labeling bottleneck to create larger datasets for models to learn from, effectively allowing models to scale up.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"A drawback of supervision is that data labeling is expensive and time-consuming.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Self-supervision differs from unsupervision. In self-supervised learning, labels are inferred from the input data. In unsupervised learning, you don\\u2019t need labels at all.\"))), mdx(\"h5\", null, \"From Large Language Models to Foundation Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"A model that can work with more than one data modality is also called a multimodal model.\"), mdx(\"img\", {\n    style: {\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"width\": \"80%\"\n    },\n    src: \"/knowledge/478c417647a2f2d21f20f4552e71318b/multi-modal-model.png\",\n    alt: \"Multi modal model\"\n  })), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Foundation models also mark the transition from task-specific models to general-purpose models. Previously, models were often developed for specific tasks, such as sentiment analysis or translation. A model trained for sentiment analysis wouldn\\u2019t be able to do translation, and vice versa.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Prompt engineering, RAG, and finetuning are three very common AI engineering techniques that you can use to adapt a model to your needs.\"))), mdx(\"h5\", null, \"From Foundation Models to AI Engineering\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AI engineering refers to the process of building applications on top of foundation models.\")), mdx(\"h4\", null, \"Foundation Model Use Cases\"), mdx(\"h2\", null, \"Bibliography\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"AI Engineering","private":false},"tableOfContents":{"items":[{"url":"#ai-engineering","title":"AI Engineering","items":[{"url":"#highlights","title":"Highlights"},{"url":"#contents","title":"Contents"},{"url":"#the-book-in-3-sentences","title":"The Book in 3 Sentences"},{"url":"#who-should-read-it","title":"Who Should Read It?"},{"url":"#my-top-3-quotes","title":"My Top 3 Quotes"},{"url":"#notes","title":"Notes","items":[{"url":"#introduction-to-building-ai-applications-with-foundation-models","title":"Introduction to Building AI Applications with Foundation Models","items":[{"url":"#the-rise-of-ai-engineering","title":"The Rise of AI Engineering","items":[{"url":"#from-language-models-to-large-language-models","title":"From Language Models to Large Language Models"},{"url":"#from-large-language-models-to-foundation-models","title":"From Large Language Models to Foundation Models"},{"url":"#from-foundation-models-to-ai-engineering","title":"From Foundation Models to AI Engineering"}]},{"url":"#foundation-model-use-cases","title":"Foundation Model Use Cases"}]}]},{"url":"#bibliography","title":"Bibliography"}]}]},"outboundReferences":[],"inboundReferences":[]},"fields":{"slug":"/3-resources/books/AI-Engineering","title":"AI Engineering"}}},"pageContext":{"id":"50903434-6baf-555b-8e1b-f4e414250f2e","refWordMdxSlugDict":{},"tocTypes":["sidebar"]}},"staticQueryHashes":["2221750479","2380733210","2768355698","63159454","847517413"]}