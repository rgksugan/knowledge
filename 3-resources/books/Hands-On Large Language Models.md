---
tags: non-fiction
type: book
author: Jay Alammar
title: Hands-On Large Language Models
sub-title: Language Understanding and Generation
---

# Hands-On Large Language Models
by [[Jay Alammar]][[Maarten Grootendorst]]

## Highlights
> Embeddings are vector representations of data that attempt to capture its meaning.

> Language AI refers to a subfield of AI that focuses on developing technologies capable of understanding, processing, and generating human language.

## Contents

### Understanding Language Models
#### An Introduction to Large Language Models
#### Tokens and Embeddings

## The Book in 3 Sentences

## Who Should Read It?

## My Top 3 Quotes

## Abbreviations
* RNN - Recurrent Neural Networks
* LMM - Large Language Models
* AI - Artificial Intelligence
* NPC - Non Playable Characters
* NLP - Natural Language Processing
* BERT - Bidirectional Encoder Representations from Transformers
* VRAM - Video Random Access Memory
* GUI - Graphical User Interface
* BPE - Byte Pair Encoding
* NER - Named Entity Recognition

## Datasets
* [Playlist](https://www.cs.cornell.edu/~shuochen/lme/data_page.html)
* [rotten_tomatoes](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes)

## Papers
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)
* [Machine Learning in Automated Text Categorization](https://arxiv.org/pdf/cs/0110053)
* [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084)
* [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)
* [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/pdf/2111.09543)
* [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085)
* [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/pdf/2402.19173)
* [StarCoder: may the source be with you!](https://arxiv.org/pdf/2305.06161)
* [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/pdf/2207.14255)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)
* [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774)
* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)
* [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/pdf/2305.13048)
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)
* [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)
* [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/pdf/2103.06874)
* [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models](https://arxiv.org/pdf/2105.13626)
* [JAPANESE AND KOREAN VOICE SEARCH](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)
* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909)
* [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226)
* [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959)

## Bibliography
* Designing Large Language Model Applications
